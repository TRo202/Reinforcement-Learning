{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rooms (RL Practice ex).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9JGXEed/GUm5nA31hZ97J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TRo202/Reinforcement-Learning/blob/main/Rooms_(RL_Practice_ex).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a simple Q-learning practice.    \n",
        "Source: [niconielsen32](https://github.com/niconielsen32)\n",
        "\n",
        "Goal: \n",
        "Given a grid of rooms  \n",
        "0 1 5(goal)     \n",
        "4 3 2   \n",
        "where rooms are connected in a way that rooms 0-4-3-1-5 are connected and rooms 2-3-1-5 are connected.\n",
        "Reach the room #5 (goal)\n",
        "\n"
      ],
      "metadata": {
        "id": "1uS7EVAmnF4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ],
      "metadata": {
        "id": "yi9-UDbxe9M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards=np.array([\n",
        "    [-1,-1,-1,-1,0,-1],\n",
        "    [-1,-1,-1,0,-1,100],\n",
        "    [-1,-1,-1,0,-1,-1],\n",
        "    [-1,0,0,-1,0,-1],\n",
        "    [0,-1,-1,0,-1,-1],\n",
        "    [-1,0,-1,-1,-1,-1]])"
      ],
      "metadata": {
        "id": "W-GMqO5Ke9kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intialize_q(m,n):\n",
        "    return np.zeros((m,n))\n",
        "q_matrix=intialize_q(6,6)"
      ],
      "metadata": {
        "id": "99C5yXO-e9tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBpjVPJ7f0lN",
        "outputId": "1c74bc8c-b1b5-4140-e977-964929f3d652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_initial_state(state=6):\n",
        "    return np.random.randint(0,state)"
      ],
      "metadata": {
        "id": "-W0u_NKdfNLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action(current_state, reward_matrix):\n",
        "    valid_actions=[]\n",
        "    for action in enumerate(reward_matrix[current_state]):  # in the current_state row of rewards_matrix, when action[1] (\"validation value\"-whether reachable)is not equal to -1, substitue valid action in action[0] (path)\n",
        "        if action [1] !=-1:  \n",
        "            valid_actions+=[action[0]]                      # for a in enumerate(j) print(a) -> returns (0, j[0]); (1, j[1]); ...; (n, j[n]) \n",
        "                                                            # why use \"random.choice()\"?  why not just \"return action[0]\"\n",
        "    return random.choice(valid_actions)                     # REASON\n",
        "                                                            # don't forget that there can be more than one choice (door) for one room"
      ],
      "metadata": {
        "id": "jxOgpLqyfNab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To take some action, we need to know the current state\n",
        "\n",
        "def take_action(current_state, reward_matrix, gamma, verbose=False):\n",
        "    action= get_action(current_state, reward_matrix)\n",
        "    sa_reward= reward_matrix[current_state, action] # current_state-action reward   # immediate reward\n",
        "    ns_reward= max(q_matrix[action,])# future new state reward                      # returns the max Q value for new state (= most recent action)\n",
        "    q_current_state= sa_reward+(gamma*ns_reward)                                    # Q value = immeidate reward + (gamma*future reward)\n",
        "    q_matrix[current_state, action]=q_current_state #matutes q_matrix\n",
        "    new_state= action                                                               # new state = most recent action\n",
        "    if verbose:\n",
        "        print(q_matrix)\n",
        "        print(f\"Old State: {current_state} | New State: {new_state}\\n\\n\")\n",
        "        if new_state== 5:\n",
        "            print(f\"Agent has reached it's goal!\")\n",
        "    return new_state\n",
        "\n",
        "def initialize_episode(reward_matrix, initial_state, gamma, verbose=False):\n",
        "    #Runs 1 episode unitl the agent reaches its goal-state\n",
        "    current_state = initial_state\n",
        "    while True: \n",
        "        current_state= take_action(current_state, reward_matrix, gamma, verbose)    # current_state = new_state (returned by take_action(...))\n",
        "        if current_state ==5:\n",
        "            break\n",
        "\n",
        "def train_agent(iteration, reward_matrix, gamma, verbose=False):\n",
        "    #Runs a given number of episodes then normalizs the matrix\n",
        "    print(\"Training in progress...\")\n",
        "    for episode in range(iteration):\n",
        "        initial_state =set_initial_state()\n",
        "        initialize_episode(reward_matrix, initial_state, gamma, verbose)\n",
        "    print(\"Training complete!\")\n",
        "    return q_matrix\n",
        "\n",
        "def normalize_matrix(q_matrix):\n",
        "    normalize_q=q_matrix/max(q_matrix[q_matrix.nonzero()])*100\n",
        "    return normalize_q.astype(int)"
      ],
      "metadata": {
        "id": "M3VRdShxfNdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test run of single episode....\n",
        "gamma=0.1\n",
        "initial_state=0\n",
        "initial_action=get_action(initial_state, rewards)\n",
        "initialize_episode(rewards, initial_state, gamma, True)\n",
        "print(q_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PINJ6c_g8lIK",
        "outputId": "a87ad9f6-2c3c-48d6-b219-5f661b780230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "Old State: 0 | New State: 4\n",
            "\n",
            "\n",
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "Old State: 4 | New State: 3\n",
            "\n",
            "\n",
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "Old State: 3 | New State: 1\n",
            "\n",
            "\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "Old State: 1 | New State: 5\n",
            "\n",
            "\n",
            "Agent has reached it's goal!\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test run of full training\n",
        "gamma=0.8\n",
        "initial_state=set_initial_state()\n",
        "initial_action=get_action(initial_state, rewards)       # why do we need \"initial_state=set_initial_state()\" and \"initial_action=get_action(initial_state, rewards)\"\n",
        "q_table=train_agent(200, rewards, gamma, False)        # if we already define initial_state in train_agent for every episode?\n",
        "print(pd.DataFrame(q_table))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5292vun-fNhd",
        "outputId": "6b2e162d-8cdc-4ea1-c4eb-027429b2cb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training in progress...\n",
            "Training complete!\n",
            "            0           1           2           3           4           5\n",
            "0    0.000000    0.000000    0.000000    0.000000  142.221391    0.000000\n",
            "1    0.000000    0.000000    0.000000  177.777352    0.000000  277.777113\n",
            "2    0.000000    0.000000    0.000000  177.777352    0.000000    0.000000\n",
            "3    0.000000  222.221690  142.221391    0.000000  142.221882    0.000000\n",
            "4  113.776738    0.000000    0.000000  177.777352    0.000000    0.000000\n",
            "5    0.000000  222.221391    0.000000    0.000000    0.000000    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def deploy_agent(init_state, q_table):\n",
        "  print(\"Start: \", init_state)\n",
        "  state = init_state\n",
        "  steps = 0\n",
        "  while True:\n",
        "    steps += 1\n",
        "    action = np.argmax(q_table[state,:])\n",
        "    print(action)\n",
        "    state = action\n",
        "    if action == 5:\n",
        "      print(\"Finished!\")\n",
        "      return steps"
      ],
      "metadata": {
        "id": "V29KLzyImgYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_room = 0\n",
        "steps = deploy_agent(start_room, q_table)\n",
        "print(\"Number of Rooms/actions: \", steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwMXtSu7ni5j",
        "outputId": "a5df8328-86c1-4d83-c052-60bd246262ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start:  0\n",
            "4\n",
            "3\n",
            "1\n",
            "5\n",
            "Finished!\n",
            "Number of Rooms/actions:  4\n"
          ]
        }
      ]
    }
  ]
}